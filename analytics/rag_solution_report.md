# RAG-бот для QuantumForge Software: анализ, сравнение стеков и рекомендации

## Постановка задачи
QuantumForge Software нуждается во внедрении RAG-бота, который:
- сокращает время поиска знаний в разрозненной базе (GitHub MD/MDX, Confluence, Google Drive, PDF, Zendesk Guide),
- автоматизирует ответы на FAQ и L2/L3 подсказки,
- выявляет пробелы в документации и помогает улучшать её качество,
- соответствует требованиям безопасности (SOC 2, внутренние политики DLP),
- масштабируется на команды/клиентов.

Ключевое ограничение: часть материалов конфиденциальна и не должна уходить в публичные облака. Нужны варианты: полностью on‑prem/приватное облако и гибрид.

Основные пользователи: разработчики, поддержка, менеджеры/аналитики, новые сотрудники. 
Заказчики: CTO/Head of Engineering, руководители Support и GRC.

## 1. Сравнение LLM-моделей для генерации
Сравниваются: 
- локальные (Hugging Face семейство, LLaMA‑3, Mistral/Mixtral, Qwen, DeepSeek)
- облачные (OpenAI GPT‑4o/4.1, YandexGPT, Anthropic Claude).

Критерии:
- Качество ответов (точность, следование инструкциям, кодогенерация, мультиязычность RU/EN, длинный контекст),
- Скорость (латентность/throughput),
- Стоимость владения/использования (TCO: inference + ops),
- Удобство/простота развёртывания.

Итоги (концентрировано):
- Качество:
  - Облачные SOTA (OpenAI GPT‑4.1/4o, Claude 3.5) дают лучшую точность, обобщение и "reasoning" на смешанных доменных данных (RU/EN), устойчивее к шуму и разночтениям версий документов.
  - Локальные 7B–13B (Mistral 7B, Llama‑3‑8B, Qwen‑7B/14B, DeepSeek‑7B) при грамотном RAG и хороших промптах дают близкое качество на узких задачах вопрос‑ответ + цитирование, но хуже в сложном рассуждении/обобщении и более чувствительны к prompt‑инженерингу и качеству ретривала.
  - 70B локальные модели повышают качество, но требуют дорогих GPU и сложнее в эксплуатации.
- Скорость:
  - Облако: латентность 300–1200 мс на короткие ответы, стабильный autoscaling.
  - Локальные: на CPU медленно; на дорогих GPU - сопоставимо с облаком, но требуют инжиниринга
- Стоимость:
  - Облако: OpEx по токенам, без капитальных затрат, но постоянные расходы.
  - Локальные: CapEx в GPU/серверы + DevOps сопровождение; выгодно при высоких объёмах или строгих ограничениях на данные.
- Развёртывание:
  - Облако: простая интеграция, быстрый time‑to‑value, сертификации/комплаенс на стороне вендора.
  - Локальные: нужно выстроить MLOps/serving, обновления весов/безопасность.

Вывод: для задач высокой критичности и приватности - локальная/гибридная модель; для быстрого MVP и лучшего качества на старте - облачная модель. 
Гибрид позволяет держать чувствительные документы локально, а нефинансовые/публичные запросы проксировать в облако с анонимизацией.

## 2. Сравнение моделей эмбеддингов
Сравниваются локальные (Sentence-Transformers) vs облачные (OpenAI embeddings).
Критерии: скорость создания индекса, качество поиска, стоимость.

Итоги:
- Качество:
  - OpenAI text‑embedding‑3‑large показывает стабильные топ‑результаты на англ. и хорошие на RU, длинный контекст, мало артефактов.
  - BGE‑m3 и multilingual‑e5 хороши в мультиязычном поиске (RU/EN), устойчивы к шумихе в корпоративных документах. Для технического домена (код/MD/PDF) bge‑large/e5‑large‑v2 дают высокую точность.
  - Для PDF и смешанных форматов помогает дообучение на дистилляции релевантности (optional).
- Скорость/индексация:
  - Локальные на GPU: высокая пропускная способность, отсутствие сетевой латентности.
  - Облачные: простота, но сетевая латентность и rate limits; для 21k+ документов это ощутимо, но параллельная индексация решает.
- Стоимость:
  - Облачные: платёж за токены при индексации и запросах. Для 18k MD/MDX + 3k Confluence страниц + 250 PDF стоимость может быть существенной на первичной индексации и регулярных реиндексах.
  - Локальные: затраты на GPU/CPU при создании индекса; при постоянном росте на 400 страниц/мес выгоднее, особенно при частой переиндексации.

Вывод: для гибридного сценария - локальная модель эмбеддингов (multilingual‑e5‑large или bge‑m3) для приватных данных; по желанию - облачные эмбеддинги для публичных источников. В критичных средах - полностью локальные эмбеддинги.

## 3. Сравнение ChromaDB и FAISS
Критерии: скорость поиска/индексации, сложность внедрения/поддержки, удобство, стоимость.

- FAISS:
  - Плюсы: эталонная скорость ANN (HNSW/IVF/Flat/PQ/OPQ), зрелость, контролируемость, гибкость. Отлично подходит для self‑hosted сервиса индекса с высоким RPS.
  - Минусы: это библиотека, а не полноценная БД. Нужны обвязка для метаданных, репликация, persistence, шардирование, ACL. Инженерные трудозатраты.
- ChromaDB:
  - Плюсы: out‑of‑the‑box векторная БД как сервис, простые API, хранение метаданных, персистентность, фильтры, коллекции, интеграции. Быстрый старт, удобна для RAG‑бота и итераций.
  - Минусы: производительность и масштабирование уступают специализированным кластерам FAISS/Weaviate/Milvus. При высоких нагрузках и строгих SLO потребуется доработка/миграция.

Скорость:
- FAISS Flat/HNSW на GPU - максимум производительности; на CPU - тоже очень быстро.
- ChromaDB поверх local persistence - достаточно для 100k–5M векторов при умеренном RPS. Для 21k–50k документов (после чанкинга 200k–500k векторов) - комфортно.

Сложность и владение:
- FAISS: потребуются отдельные сервисы (метаданные, индекс‑шардинг, снапшоты, WAL/backup, REST/gRPC слой), DevOps.
- ChromaDB: один сервис/контейнер, бэкапы уровня диска/volume, простые апдейты.

Вывод: стартовать с ChromaDB для скорости внедрения и удобства; иметь путь роста к FAISS‑кластеру или Milvus/Weaviate при росте объёма и требований к SLO.


## 4. Рекомендуемая конфигурация серверов
Нужны два окружения: MVP/PoC и прод.

- PoC (on‑prem или приватный кластер в EKS):
  - CPU: 8 vCPU
  - RAM: 32–64 GB
  - GPU: не обязательно; при локальном LLM - 1×RTX 6000 Ada 48GB или A10/A100 40GB
  - Диски: 500 GB NVMe (индексы + кэш документов)
  - Использование: локальные эмбеддинги, ChromaDB, внешняя LLM (OpenAI/YandexGPT) через прокси.

- Prod (гибрид):
  - Вариант A (локальные эмбеддинги + ChromaDB, LLM облачная):
    - CPU: 16–24 vCPU
    - RAM: 64–128 GB
    - GPU: нет
    - Диски: 1–2 TB NVMe (для 1–5M векторов с ростом)
    - Горизонтальное масштабирование по ChromaDB read‑replicas.
  - Вариант B (полностью локально, включая LLM 8–13B):
    - CPU: 24–32 vCPU
    - RAM: 128 GB
    - GPU: 1×A100 80GB или 2×A10 24GB/RTX 6000 Ada
    - Диски: 1–2 TB NVMe

Сетевые требования: выделенный ingress, mTLS между сервисами, доступ к Confluence/Google Drive через сервисные аккаунты/коннекторы.

## 5. Варианты архитектуры и рекомендации

Вариант 1. Облако‑first (быстрый MVP)
- LLM: OpenAI GPT‑4.1/4o
- Embeddings: OpenAI text‑embedding‑3‑large
- Векторная БД: ChromaDB self‑hosted (или облачная альтернатива, если комплаенс допускает)
- Плюсы: максимальное качество ответов и быстрый запуск, минимум DevOps
- Минусы: данные/эмбеддинги уходят в облако; затраты по токенам/индексации
- Применимость: не для всех внутренних документов, подходит для публичных/мало‑чувствительных разделов

Вариант 2. Гибрид
- LLM: внешняя для сложных запросов через privacy‑proxy с анонимизацией; локальная 7B–13B (Llama‑3‑8B‑Instruct / Mistral‑7B‑Instruct / Qwen‑14B) для чувствительных запросов
- Embeddings: локальные (multilingual‑e5‑large или bge‑m3) для приватных данных; при желании - облачные для открытых источников
- Векторная БД: ChromaDB self‑hosted в EKS/RDS‑volume с регулярными снапшотами; путь миграции к FAISS/Milvus при росте
- Безопасность: PII scrubber, DLP‑политики, шифрование at‑rest (LUKS/EBS encryption) и in‑transit (mTLS), разграничение пространств данных
- Плюсы: соблюдение комплаенса, контроль затрат, хорошее качество
- Минусы: выше сложность, чем у чистого облака

Вариант 3. Полностью on‑prem
- LLM: локальная (13B/70B, в зависимости от качества); сервер через vLLM/Triton
- Embeddings: локальные (bge‑m3/multilingual‑e5)
- Векторная БД: FAISS + собственная обвязка или Milvus/Weaviate (готовые фичи: шардинг, ACL, бэкапы)
- Плюсы: максимальная приватность, предсказуемые затраты при большом трафике
- Минусы: наивысшая сложность внедрения и сопровождения; риск качества по сравнению с SOTA облаком

Заключение: Вариант 2 (Гибрид) как основной путь. 
Вариант 1 - для ускоренного пилота на непубличных данных с согласием GRC. 
Вариант 3 - только если политика полностью исключает внешние вызовы.

## 6. План внедрения 
1) Инвентаризация источников и политика данных (DLP матрица, теги чувствительности). 
2) Выбор эмбеддингов (bge‑m3 или multilingual‑e5), построение индекса в ChromaDB, фильтрация по метаданным (owner, service, version, confidentiality). 
3) LLM‑оркестрация: роутинг запросов (local vs cloud), Guardrails, цитирование, критические ответы с верификацией.
4) Observability: quality metrics, latency, cost, feedback‑loop.
5) Gap analysis: неотвеченные/низкоуверенные вопросы → задачи в Jira/Confluence с владельцем.
6) Безопасность: mTLS, OAuth/SAML SSO, RBAC/ABAC, аудиты.
7) Эксплуатация: бэкапы индексов, периодический re‑embed, CI/CD (GitHub Actions), Terraform/Helm.

## 7. Итоговые рекомендации по инструментам
- Векторная БД: стартовать с ChromaDB (простота, достаточная производительность, есть фильтрация/персистентность). Установить SLO и триггеры миграции (объём > 5M векторов, RPS > 100, P95 > 800 мс) → миграция на Milvus/Weaviate или кастомный FAISS‑кластер.
- Эмбеддинги: локальные multilingual‑e5‑large или bge‑m3. Для части публичных данных - OpenAI text‑embedding‑3‑large при необходимости.
- LLM: гибридный роутер. По умолчанию - облачная GPT‑4.1/4o; для чувствительных запросов/данных - локальная Llama‑3‑8B/Mistral‑7B с quantization и vLLM.
- Инфраструктура: EKS (уже используется), Helm, Prometheus/Grafana, Loki/ELK.
